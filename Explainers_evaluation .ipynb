{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "Most important word position \n",
    "    \n",
    "    \n",
    "Explainers agreement\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C://Users//idobotzer//Technion//Inteligent Interactive general//Interactive Inteligence/explainers_df.csv\"\n",
    "explainers_df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainers_df['lime_scores'] = explainers_df['lime_scores'].apply(literal_eval)\n",
    "explainers_df['erase_scores'] = explainers_df['erase_scores'].apply(literal_eval)\n",
    "explainers_df['saliency_scores'] = explainers_df['saliency_scores'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_in_sentence(scores):\n",
    "    max_index =  np.argmax(scores)\n",
    "    if max_index < len(scores)/3:\n",
    "        return \"Beginning\"\n",
    "    if max_index < (len(scores)/3)*2:\n",
    "        return \"Middle\"\n",
    "    return \"Ending\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positions_analysis(scores_df):\n",
    "    lime_dict = {'Beginning':0, 'Middle':0, 'Ending':0}\n",
    "    saliency_dict = {'Beginning':0, 'Middle':0, 'Ending':0}\n",
    "    erase_dict = {'Beginning':0, 'Middle':0, 'Ending':0}   \n",
    "    for index, row in explainers_df.iterrows():\n",
    "        lime = row['lime_scores']\n",
    "        saliency = row['saliency_scores']\n",
    "        erase = row['erase_scores']\n",
    "        lime_dict[position_in_sentence(lime)]+=1\n",
    "        saliency_dict[position_in_sentence(saliency)]+=1\n",
    "        erase_dict[position_in_sentence(erase)]+=1\n",
    "    return lime_dict,saliency_dict,erase_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_dict, saliency_dict, erase_dict = positions_analysis(explainers_df)\n",
    "plt.bar(*zip(*lime_dict.items()))\n",
    "plt.title('LIME')\n",
    "plt.show()\n",
    "plt.bar(*zip(*saliency_dict.items()))\n",
    "plt.title('Saliency')\n",
    "plt.show()\n",
    "plt.bar(*zip(*erase_dict.items()))\n",
    "plt.title('Erase')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k__m_matches(scores_df,k,m):\n",
    "    row_count = scores_df.shape[0]\n",
    "    lime_saliency_count = 0\n",
    "    lime_erase_count = 0\n",
    "    saliency_erase_count = 0\n",
    "    for index, row in explainers_df.iterrows():\n",
    "        top3_lime = extract_top_k(row['lime_scores'],k)\n",
    "        top3_saliency = extract_top_k(row['saliency_scores'],k)\n",
    "        top3_erase = extract_top_k(row['erase_scores'],k)\n",
    "        lime_saliency_count += len(list(set(top3_lime) & set(top3_saliency)))>=m\n",
    "        lime_erase_count += len(list(set(top3_lime) & set(top3_erase)))>=m\n",
    "        saliency_erase_count += len(list(set(top3_saliency) & set(top3_erase)))>=m\n",
    "    return lime_saliency_count/row_count, lime_erase_count/row_count, saliency_erase_count/row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_k(scores, k):\n",
    "    sort = sorted([(x,i) for (i,x) in enumerate(scores)], reverse=True )[:k]\n",
    "    return [item[1] for item in sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order(lst):\n",
    "    s = sorted(lst, reverse=True)\n",
    "    return [s.index(i) + 1 for i in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_saliency, lime_erase, saliency_erase = top_k__m_matches(explainers_df,1,1)\n",
    "print(lime_saliency, lime_erase, saliency_erase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman(scores_df):\n",
    "    spearman_scores = []\n",
    "    lime_saliency_count = 0\n",
    "    lime_erase_count = 0\n",
    "    saliency_erase_count = 0\n",
    "    for index, row in explainers_df.iterrows():\n",
    "        lime = list(row['lime_scores'])\n",
    "        saliency = list(row['saliency_scores'])\n",
    "        erase = list(row['erase_scores'])\n",
    "        spearman_scores.append([scipy.stats.spearmanr(lime,saliency)[0],scipy.stats.spearmanr(lime,erase)[0], scipy.stats.spearmanr(saliency,erase)[0]])\n",
    "    return spearman_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_scores=spearman(explainers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_saliency=[item[0] for item in spearman_scores]\n",
    "print(sum(lime_saliency)/len(lime_saliency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_erase=[item[1] for item in spearman_scores]\n",
    "print(sum(lime_erase)/len(lime_erase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saleincy_erase=[item[2] for item in spearman_scores]\n",
    "print(sum(saleincy_erase)/len(saleincy_erase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_dict = {0:0, 1:0, 2:0}\n",
    "for i,sentence in enumerate(spearman_scores):\n",
    "    max_index =  np.argmax(sentence)\n",
    "    corr_dict[max_index]+=1/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
