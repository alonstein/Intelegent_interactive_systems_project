{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import itertools\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.utils.data.dataset import Dataset, TensorDataset\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "\n",
    "from torchtext import data   \n",
    "\n",
    "UNKNOWN_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "SPECIAL_TOKENS = [UNKNOWN_TOKEN, PAD_TOKEN]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('subjectivity_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_words = df['text'].apply(lambda x:x.split())\n",
    "lengths = splitted_words.apply(lambda x:len(x))\n",
    "splitted_words = sum(splitted_words, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(splitted_words))\n",
    "words_map = {word:i for i,word in enumerate(vocab)}\n",
    "words_map[UNKNOWN_TOKEN] = len(words_map)\n",
    "words_map[PAD_TOKEN] = len(words_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    # Read input file and get sentence\n",
    "    def __init__(self, file_path, words_dict):\n",
    "        self.file_path = file_path\n",
    "        self.words_dict = words_dict\n",
    "        self.get_sentences()\n",
    "    \n",
    "    def sentence_to_list_of_nums(self, sentence):\n",
    "        words = sentence.split()\n",
    "        new_sentence = [self.words_dict[x] if x in self.words_dict else self.words_dict[UNKNOWN_TOKEN] for x in words]\n",
    "        return new_sentence\n",
    "    \n",
    "    def get_sentences(self):\n",
    "        df = pd.read_csv(self.file_path)\n",
    "        df['text'] = df['text'].apply(lambda x:self.sentence_to_list_of_nums(x))\n",
    "        self.labels = list(df['label'])\n",
    "        self.sentences = list(df['text'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parseDataset(Dataset):\n",
    "    def __init__(self, word_dict, file, padding=False, word_embeddings=None):\n",
    "        super().__init__()\n",
    "        self.file = file\n",
    "        self.datareader = DataReader(self.file, word_dict)\n",
    "        self.word_vocab_size = len(self.datareader.words_dict)\n",
    "  \n",
    "        if word_embeddings:\n",
    "            self.word_idx_mappings, self.idx_word_mappings, self.word_vectors = word_embeddings\n",
    "        else:\n",
    "            self.word_idx_mappings, self.idx_word_mappings, self.word_vectors = self.init_word_embeddings(word_dict)\n",
    "\n",
    "        #self.unknown_idx = self.word_idx_mappings.get(UNKNOWN_TOKEN)\n",
    "        self.word_vector_dim = self.word_vectors.size(-1)\n",
    "        \n",
    "        self.sentence_lens = [len(sentence) for sentence in self.datareader.sentences]\n",
    "        self.max_seq_len = max(self.sentence_lens)\n",
    "        self.sentences_dataset = self.convert_sentences_to_dataset(padding)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences_dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        word_embed_idx,label = self.sentences_dataset[index]\n",
    "        return word_embed_idx, label\n",
    "\n",
    "    @staticmethod\n",
    "    def init_word_embeddings(word_dict):\n",
    "        glove = Vocab(Counter(word_dict), vectors=\"glove.6B.100d\", specials=SPECIAL_TOKENS)\n",
    "        return glove.stoi, glove.itos, glove.vectors\n",
    "    \n",
    "    def get_word_embeddings(self):\n",
    "        return self.word_idx_mappings, self.idx_word_mappings, self.word_vectors\n",
    "\n",
    "    def convert_sentences_to_dataset(self, padding):\n",
    "        sentence_word_idx_list = []\n",
    "        sentence_len_list = []\n",
    "        labels = []\n",
    "        for i in range(len(self.datareader.sentences)):\n",
    "            words_idx_list = self.datareader.sentences[i]\n",
    "            label = self.datareader.labels[i]\n",
    "            sentence_len = len(words_idx_list)\n",
    "            sentence_word_idx_list.append(torch.tensor(words_idx_list, dtype=torch.long, requires_grad=False))\n",
    "            labels.append(torch.tensor(label, dtype=torch.float, requires_grad=False))\n",
    "            sentence_len_list.append(sentence_len)\n",
    "    \n",
    "        return {i: sample_tuple for i, sample_tuple in enumerate(zip(sentence_word_idx_list,\n",
    "                                                                     labels))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, hidden_dim, word_vocab_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.word_embedding = nn.Embedding(word_vocab_size,word_embedding_dim)\n",
    "        self.emb_dim = word_embedding_dim\n",
    "        self.lstm = nn.LSTM(input_size=word_embedding_dim,bidirectional=False, hidden_size=hidden_dim, num_layers=1,batch_first=True )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        word_idx_tensor = sentence\n",
    "        word_embeds = self.word_embedding(word_idx_tensor.to(self.device))\n",
    "        output, (hidden,_) = self.lstm(word_embeds)\n",
    "        hidden = hidden.view(-1)\n",
    "        out = self.fc(hidden)\n",
    "        out =  self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WORD_EMBEDDING_DIM = 20\n",
    "HIDDEN_DIM = 50\n",
    "train = parseDataset(words_map,'subjectivity_train.csv')\n",
    "train_dataloader = DataLoader(train, shuffle=True)\n",
    "word_vocab_size = train.word_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = parseDataset(words_map,'subjectivity_test.csv')\n",
    "test_dataloader = DataLoader(test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('words_dict_subjectivity3.pkl', 'wb') as f:\n",
    "    pickle.dump(words_map, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(WORD_EMBEDDING_DIM, HIDDEN_DIM, word_vocab_size)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7157762413576367\n",
      "0.9024512884978001\n",
      "0.9637963544940289\n",
      "0.9891891891891892\n",
      "0.995223130106851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "acumulate_grad_steps = 50 \n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "epochs = 5\n",
    "loss = 0\n",
    "loss_function = nn.BCELoss()\n",
    "epoch_loss = []\n",
    "epoch_accuracy = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    acc = 0 #to keep track of accuracy\n",
    "    loss = 0 # To keep track of the loss value\n",
    "    i = 0\n",
    "    total_loss = []\n",
    "    scores = []\n",
    "    true = []\n",
    "    for sentence,label in train_dataloader:\n",
    "        i += 1\n",
    "        \n",
    "        score = model(sentence)\n",
    "        loss = loss_function(score, label)\n",
    "        loss = loss / acumulate_grad_steps\n",
    "        loss.backward()\n",
    "        if i % acumulate_grad_steps == 0:\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "        total_loss.append(loss.item()) \n",
    "        scores.append(score)\n",
    "        true.append(label)\n",
    "    \n",
    "    predictions = [1 if score>0.5 else 0 for score in scores]\n",
    "    accuracy = accuracy_score(predictions,true)\n",
    "    epoch_accuracy.append(accuracy)\n",
    "    mean_loss = sum(total_loss)/len(total_loss)\n",
    "    epoch_loss.append(mean_loss)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9942174732872407\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "for sentence,label in test_dataloader:\n",
    "    score = model(sentence)\n",
    "    scores.append(score)\n",
    "    true.append(label)\n",
    "predictions = [1 if score>0.5 else 0 for score in scores]\n",
    "accuracy = accuracy_score(predictions,true)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'subjectivity_model3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
